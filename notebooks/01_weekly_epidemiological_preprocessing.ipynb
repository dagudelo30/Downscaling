{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c3e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0dbdbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### funciones\n",
    "\n",
    "def start_of_epi_year(Y: int) -> pd.Timestamp:\n",
    "    dec29 = pd.Timestamp(Y - 1, 12, 29)\n",
    "    jan4  = pd.Timestamp(Y, 1, 4)\n",
    "    w = dec29.weekday()\n",
    "    offset = (6 - w) % 7\n",
    "    first_sunday = dec29 + pd.Timedelta(days=offset)\n",
    "    if first_sunday > jan4:\n",
    "        first_sunday -= pd.Timedelta(days=7)\n",
    "    return first_sunday\n",
    "\n",
    "\n",
    "def get_epi_year_week(date: pd.Timestamp) -> tuple[int, int]:\n",
    "    date = pd.Timestamp(date)\n",
    "    # ‚Äúdomingo epidemiol√≥gico‚Äù de la fecha\n",
    "    offset = (date.weekday() + 1) % 7          # 0 = domingo\n",
    "    sunday = date - pd.Timedelta(days=offset)\n",
    "\n",
    "    next_start = start_of_epi_year(sunday.year + 1)\n",
    "    epi_year   = sunday.year if sunday < next_start else sunday.year + 1\n",
    "\n",
    "    start_year = start_of_epi_year(epi_year)\n",
    "    epi_week   = ((sunday - start_year).days // 7) + 1\n",
    "    return epi_year, epi_week\n",
    "\n",
    "\n",
    "def process_chirps(path: str,\n",
    "                   start_date: str | pd.Timestamp,\n",
    "                   end_date: str | pd.Timestamp,\n",
    "                   path_out: str,\n",
    "                   crs: str = \"EPSG:4326\"\n",
    "                   ) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Lee GeoTIFFs diarios CHIRPS entre start_date y end_date.\n",
    "    Si falta un d√≠a, lo rellena con NaN.\n",
    "    Devuelve un DataArray con dims (\"time\", \"y\", \"x\") y coords extra: epi_year, epi_week.\n",
    "    Guarda tambi√©n archivo NetCDF semanal.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    fechas = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "\n",
    "    data = []\n",
    "    template_da = None\n",
    "\n",
    "    print(f\"üì• Procesando {len(fechas)} archivos desde: {path}\")\n",
    "\n",
    "    for dt in tqdm(fechas, desc=\"Procesando GeoTIFFs CHIRPS\", unit=\"d√≠a\"):\n",
    "        fname = f\"valle_{dt.strftime('%Y-%m-%d')}.tif\"\n",
    "        fpath = os.path.join(path, fname)\n",
    "\n",
    "        if os.path.exists(fpath):\n",
    "            da = rioxarray.open_rasterio(fpath).squeeze()\n",
    "            template_da = da if template_da is None else template_da\n",
    "        else:\n",
    "            if template_da is None:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"No encontr√© {fname} ni ning√∫n otro TIFF en el rango \"\n",
    "                    \"para usar como plantilla de tama√±o/proyecci√≥n.\"\n",
    "                )\n",
    "            da = xr.full_like(template_da, np.nan)\n",
    "\n",
    "        da.rio.write_crs(crs, inplace=True)\n",
    "        da = da.assign_coords(time=dt).expand_dims(\"time\")\n",
    "        da.name = \"precip\"\n",
    "        data.append(da)\n",
    "\n",
    "    chirps = xr.concat(data, dim=\"time\")\n",
    "\n",
    "    # Asignar semana epidemiol√≥gica\n",
    "    epi_weeks = []\n",
    "    for ts in chirps.time.dt.floor(\"D\").values:\n",
    "        epi_year, epi_week = get_epi_year_week(ts)\n",
    "        epi_weeks.append(f\"{epi_year}-{epi_week:02d}\")\n",
    "\n",
    "    chirps_epi = chirps.assign_coords(epi_week=(\"time\", epi_weeks))\n",
    "    weekly_sum = chirps_epi.groupby(\"epi_week\").sum(\"time\")\n",
    "    ds_final = weekly_sum.to_dataset(name=\"precip\")\n",
    "\n",
    "    # Determinar nombre de archivo basado en rango de semanas\n",
    "    epi_tuples = pd.Series(epi_weeks).str.extract(r'(?P<year>\\d{4})-(?P<week>\\d{2})').astype(int)\n",
    "    epi_tuples_list = list(zip(epi_tuples['year'], epi_tuples['week']))\n",
    "    epi_tuples_sorted = sorted(epi_tuples_list)\n",
    "    epi_min = f\"{epi_tuples_sorted[0][0]}-{epi_tuples_sorted[0][1]:02d}\"\n",
    "    epi_max = f\"{epi_tuples_sorted[-1][0]}-{epi_tuples_sorted[-1][1]:02d}\"\n",
    "    \n",
    "    filename = os.path.join(path_out, f\"chirps_{epi_min}_{epi_max}.nc\") \n",
    "    ds_final.to_netcdf(filename, format=\"NETCDF4\", engine=\"netcdf4\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Archivo guardado: {filename}\")\n",
    "    print(f\"üïí Tiempo total: {elapsed:.2f} segundos ({elapsed/60:.2f} minutos)\")\n",
    "\n",
    "    return ds_final\n",
    "\n",
    "def process_c3s(\n",
    "    path: str,\n",
    "    start_date: str | pd.Timestamp,\n",
    "    end_date: str | pd.Timestamp,\n",
    "    vars: dict[str, str],           # {'tmean': '2M_TEMPERATURE_MEAN', ...}\n",
    "    path_out: str,\n",
    "    crs: str = \"EPSG:4326\",\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Une d√≠a por d√≠a los NetCDF de C3S que est√°n separados por carpeta/variable.\n",
    "    Devuelve un Dataset con todas las variables concatenadas a lo largo de 'time'.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    path = Path(path)\n",
    "    fechas = pd.date_range(pd.Timestamp(start_date), pd.Timestamp(end_date), freq=\"D\")\n",
    "\n",
    "    diarios = []\n",
    "    print(f\"üì• Procesando {len(fechas)} fechas entre {start_date} y {end_date} desde: {path}\")\n",
    "    \n",
    "    for dt in tqdm(fechas, desc=\"Procesando archivos C3S\", unit=\"d√≠a\"):\n",
    "        fecha_str = dt.strftime(\"%Y%m%d\")\n",
    "        datasets_del_dia = []\n",
    "\n",
    "        for alias, carpeta in vars.items():\n",
    "            folder = path / carpeta\n",
    "            patron = f\"*{fecha_str}*\"\n",
    "            archivos = list(folder.rglob(patron))\n",
    "\n",
    "            if not archivos:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"‚õî No se encontr√≥ archivo con fecha {fecha_str} en {folder}\"\n",
    "                )\n",
    "\n",
    "            ds = xr.open_dataset(archivos[0], decode_coords=\"all\")\n",
    "            var_name = list(ds.data_vars)[0]\n",
    "            ds = ds.rename({var_name: alias})\n",
    "\n",
    "            if \"time\" not in ds.dims:\n",
    "                ds = ds.expand_dims(time=[dt])\n",
    "            else:\n",
    "                ds = ds.assign_coords(time=(\"time\", [dt]))\n",
    "\n",
    "            datasets_del_dia.append(ds)\n",
    "\n",
    "        ds_diario = xr.merge(datasets_del_dia, combine_attrs=\"override\")\n",
    "        diarios.append(ds_diario)\n",
    "\n",
    "    datos = xr.concat(diarios, dim=\"time\").sortby(\"time\")\n",
    "\n",
    "    # Asignar semanas epidemiol√≥gicas\n",
    "    epi_weeks = []\n",
    "    for ts in datos.time.dt.floor(\"D\").values:\n",
    "        epi_year, epi_week = get_epi_year_week(ts)\n",
    "        epi_weeks.append(f\"{epi_year}-{epi_week:02d}\")\n",
    "\n",
    "    datos_epi = datos.assign_coords(epi_week=(\"time\", epi_weeks))\n",
    "    c3s_mean = datos_epi.groupby([\"epi_week\"]).mean(\"time\")\n",
    "\n",
    "    if crs:\n",
    "        try:\n",
    "            c3s_mean = c3s_mean.rio.write_crs(crs, inplace=False)\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    # Obtener rango de semanas para el nombre del archivo\n",
    "    epi_tuples = pd.Series(epi_weeks).str.extract(r'(?P<year>\\d{4})-(?P<week>\\d{2})').astype(int)\n",
    "    epi_tuples_list = list(zip(epi_tuples['year'], epi_tuples['week']))\n",
    "    epi_tuples_sorted = sorted(epi_tuples_list)\n",
    "    epi_min = f\"{epi_tuples_sorted[0][0]}-{epi_tuples_sorted[0][1]:02d}\"\n",
    "    epi_max = f\"{epi_tuples_sorted[-1][0]}-{epi_tuples_sorted[-1][1]:02d}\"\n",
    "    \n",
    "    filename = os.path.join(path_out, f\"c3s_{epi_min}_{epi_max}.nc\") \n",
    "    c3s_mean.to_netcdf(filename, format=\"NETCDF4\", engine=\"netcdf4\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Archivo guardado: {filename}\")\n",
    "    print(f\"üïí Tiempo total: {elapsed:.2f} segundos ({elapsed/60:.2f} minutos)\")\n",
    "\n",
    "    return c3s_mean\n",
    "\n",
    "\n",
    "def process_gee(\n",
    "    path: Path,\n",
    "    files: list[Path],\n",
    "    shape: tuple[int, int],\n",
    "    crs: str = \"EPSG:4326\"\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Procesa una lista de archivos GeoTIFF diarios (NDVI, NDWI, etc.) provenientes de GEE.\n",
    "    Agrega coordenadas temporales y CRS, luego agrega semanalmente por semana epidemiol√≥gica.\n",
    "    \n",
    "    Par√°metros:\n",
    "    - path: Carpeta contenedora (solo usada para nombrar la variable final).\n",
    "    - files: Lista de archivos GeoTIFF (uno por d√≠a).\n",
    "    - shape: Dimensiones esperadas de los datos (alto, ancho).\n",
    "    - crs: Sistema de referencia de coordenadas a usar.\n",
    "\n",
    "    Retorna:\n",
    "    - xr.DataArray con dimensi√≥n 'epi_week' y nombre igual al nombre de la carpeta.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(f\"No se proporcionaron archivos para {path}\")\n",
    "\n",
    "    print(f\"üìÇ Procesando {len(files)} archivos desde: {path}\")\n",
    "\n",
    "    # Obtener shape y coordenadas de referencia\n",
    "    ref_coords_found = False\n",
    "    for f_ref in files:\n",
    "        try:\n",
    "            ds_ref = rioxarray.open_rasterio(f_ref).squeeze(\"band\", drop=True)\n",
    "            if ds_ref.shape == shape:\n",
    "                ref_x, ref_y = ds_ref.x.values, ds_ref.y.values\n",
    "                ref_coords_found = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {f_ref.name}: {e}\")\n",
    "\n",
    "    if not ref_coords_found:\n",
    "        raise RuntimeError(\"No se encontr√≥ archivo con shape v√°lido para usar como referencia.\")\n",
    "\n",
    "    datasets = []\n",
    "    archivos_excluidos = []\n",
    "\n",
    "    for f in tqdm(files, desc=\"Procesando GeoTIFFs GEE\", unit=\"archivo\"):\n",
    "        fecha = extraer_fecha_individual(f)\n",
    "        try:\n",
    "            ds = rioxarray.open_rasterio(f, chunks={\"y\": 200, \"x\": 200}).squeeze(\"band\", drop=True)\n",
    "            if ds.shape == shape:\n",
    "                ds = ds.reset_coords(drop=True)\n",
    "                ds = ds.assign_coords({\"x\": ref_x, \"y\": ref_y})\n",
    "                ds = ds.expand_dims(time=[fecha])\n",
    "                datasets.append(ds)\n",
    "            else:\n",
    "                print(f\"‚õî Excluido (dimensiones incorrectas {ds.shape}): {f.name}\")\n",
    "                archivos_excluidos.append(f.name)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {f.name}: {e}\")\n",
    "            archivos_excluidos.append(f.name)\n",
    "\n",
    "    if not datasets:\n",
    "        raise RuntimeError(\"No se pudo cargar ning√∫n archivo con dimensiones v√°lidas.\")\n",
    "\n",
    "    da_concat = xr.concat(datasets, dim=\"time\")\n",
    "\n",
    "    epi_weeks = []\n",
    "    for ts in da_concat.time.dt.floor(\"D\").values:\n",
    "        epi_year, epi_week = get_epi_year_week(ts)\n",
    "        epi_weeks.append(f\"{epi_year}-{epi_week:02d}\")\n",
    "\n",
    "    datos_epi = da_concat.assign_coords(epi_week=(\"time\", epi_weeks))\n",
    "    datos_mean = datos_epi.groupby(\"epi_week\").mean(\"time\")\n",
    "\n",
    "    if crs:\n",
    "        try:\n",
    "            datos_mean = datos_mean.rio.write_crs(crs, inplace=False)\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    variable_name = path.name\n",
    "    datos_mean.name = variable_name\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ {variable_name}: {len(datasets)} archivos incluidos, {len(archivos_excluidos)} excluidos.\")\n",
    "    print(f\"üïí Tiempo total: {elapsed:.2f} segundos ({elapsed/60:.2f} minutos)\")\n",
    "\n",
    "    return datos_mean\n",
    "\n",
    "\n",
    "def extraer_fecha_individual(f: Path) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Extrae la fecha desde el nombre del archivo, buscando un patr√≥n YYYY-MM-DD o YYYYMMDD.\n",
    "    Devuelve un pd.Timestamp.\n",
    "    \"\"\"\n",
    "    fname = f.name\n",
    "    # Intenta YYYY-MM-DD\n",
    "    match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", fname)\n",
    "    if match:\n",
    "        return pd.to_datetime(match.group(0))\n",
    "    \n",
    "    # Intenta YYYYMMDD\n",
    "    match = re.search(r\"\\d{8}\", fname)\n",
    "    if match:\n",
    "        return pd.to_datetime(match.group(0), format=\"%Y%m%d\")\n",
    "    \n",
    "    raise ValueError(f\"No se pudo extraer fecha del nombre: {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b900944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Procesando 4263 archivos desde: D:/OneDrive - CGIAR/Desktop/codigos_dengue/climate_data_downloader/data/processed/chirps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs CHIRPS: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4263/4263 [01:01<00:00, 69.13d√≠a/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Archivo guardado: D:/OneDrive - CGIAR/Desktop/downscaling/processed/chirps\\chirps_2013-22_2025-05.nc\n",
      "üïí Tiempo total: 67.01 segundos (1.12 minutos)\n",
      "<xarray.Dataset> Size: 4MB\n",
      "Dimensions:      (x: 36, y: 40, epi_week: 610)\n",
      "Coordinates:\n",
      "    band         int64 8B 1\n",
      "  * x            (x) float64 288B -77.47 -77.42 -77.37 ... -75.82 -75.77 -75.72\n",
      "  * y            (y) float64 320B 5.025 4.975 4.925 4.875 ... 3.175 3.125 3.075\n",
      "    spatial_ref  int64 8B 0\n",
      "  * epi_week     (epi_week) object 5kB '2013-22' '2013-23' ... '2025-05'\n",
      "Data variables:\n",
      "    precip       (epi_week, y, x) float32 4MB 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# ---------- Agregaci√≥n semana epidemiologica CHIRPS ----------\n",
    "\n",
    "CHIRPS_IN = r\"D:/OneDrive - CGIAR/Desktop/codigos_dengue/climate_data_downloader/data/processed/chirps\"\n",
    "CHIRPS_OUT = r\"D:/OneDrive - CGIAR/Desktop/downscaling/data/preprocessed/chirps\"\n",
    "chirps = process_chirps(CHIRPS_IN, \"2013-06-01\", \"2025-01-31\",CHIRPS_OUT)\n",
    "print(chirps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbe0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Procesando 4263 fechas entre 2013-06-01 y 2025-01-31 desde: D:\\OneDrive - CGIAR\\Desktop\\codigos_dengue\\climate_data_downloader\\data\\processed\\copernicus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos C3S: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4263/4263 [13:50<00:00,  5.13d√≠a/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Archivo guardado: D:/OneDrive - CGIAR/Desktop/downscaling/processed/copernicus\\c3s_2013-22_2025-05.nc\n",
      "üïí Tiempo total: 901.42 segundos (15.02 minutos)\n",
      "<xarray.Dataset> Size: 3MB\n",
      "Dimensions:      (epi_week: 610, lat: 20, lon: 17)\n",
      "Coordinates:\n",
      "  * lon          (lon) float64 136B -77.4 -77.3 -77.2 ... -76.0 -75.9 -75.8\n",
      "  * lat          (lat) float64 160B 5.0 4.9 4.8 4.7 4.6 ... 3.5 3.4 3.3 3.2 3.1\n",
      "  * epi_week     (epi_week) object 5kB '2013-22' '2013-23' ... '2025-05'\n",
      "    spatial_ref  int64 8B 0\n",
      "Data variables:\n",
      "    tmax         (epi_week, lat, lon) float32 830kB 301.0 302.7 ... 287.0 290.3\n",
      "    tmin         (epi_week, lat, lon) float32 830kB 296.5 295.1 ... 280.5 282.8\n",
      "    humidity     (epi_week, lat, lon) float32 830kB 82.11 86.37 ... 92.78 91.18\n",
      "    windspeed    (epi_week, lat, lon) float32 830kB 2.703 1.769 ... 1.564 1.246\n",
      "Attributes:\n",
      "    CDI:          Climate Data Interface version 1.9.2 (http://mpimet.mpg.de/...\n",
      "    history:      Fri Mar 12 15:16:22 2021: cdo splitday /archive/ESG/wit015/...\n",
      "    Conventions:  CF-1.7\n",
      "    CDO:          Climate Data Operators version 1.9.2 (http://mpimet.mpg.de/...\n"
     ]
    }
   ],
   "source": [
    "# ---------- Agregaci√≥n semana epidemiologica Copernicus ----------\n",
    "C3S_VARIABLES = {\n",
    "    'tmax' : '2M_TEMPERATURE_MAXIMUM',\n",
    "    'tmin' : '2M_TEMPERATURE_MINIMUM',\n",
    "    'humidity': '2M_RELATIVE_HUMIDITY',\n",
    "    'windspeed': '10M_WIND_SPEED'}\n",
    "C3S_INPUT = \"D:/OneDrive - CGIAR/Desktop/codigos_dengue/climate_data_downloader/data/processed/copernicus\"\n",
    "C3S_OUTPUT= \"D:/OneDrive - CGIAR/Desktop/downscaling/data/preprocessed/copernicus\"\n",
    "c3s  = process_c3s(C3S_INPUT,\"2013-06-01\", \"2025-01-31\",C3S_VARIABLES,C3S_OUTPUT)\n",
    "print(c3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8896fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Procesando 4242 archivos desde: D:\\OneDrive - CGIAR\\Desktop\\codigos_dengue\\climate_data_downloader\\data\\processed\\gee\\NDVI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs GEE:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1943/4242 [00:35<00:41, 55.78archivo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚õî Excluido (dimensiones incorrectas (433, 421)): NDVI_MODIS_2018-09-27.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs GEE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4242/4242 [01:19<00:00, 53.67archivo/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ NDVI: 4241 archivos incluidos, 1 excluidos.\n",
      "üïí Tiempo total: 82.48 segundos (1.37 minutos)\n",
      "üìÇ Procesando 4241 archivos desde: D:\\OneDrive - CGIAR\\Desktop\\codigos_dengue\\climate_data_downloader\\data\\processed\\gee\\NDWI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs GEE:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1946/4241 [00:35<00:38, 59.79archivo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚õî Excluido (dimensiones incorrectas (433, 421)): NDWI_MODIS_2018-09-27.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs GEE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4241/4241 [01:17<00:00, 55.05archivo/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ NDWI: 4240 archivos incluidos, 1 excluidos.\n",
      "üïí Tiempo total: 79.42 segundos (1.32 minutos)\n",
      "<xarray.Dataset> Size: 889MB\n",
      "Dimensions:      (x: 422, y: 433, epi_week: 608)\n",
      "Coordinates:\n",
      "  * x            (x) float64 3kB -8.601e+06 -8.6e+06 ... -8.391e+06 -8.39e+06\n",
      "  * y            (y) float64 3kB 5.598e+05 5.593e+05 ... 3.443e+05 3.438e+05\n",
      "  * epi_week     (epi_week) object 5kB '2013-22' '2013-23' ... '2025-05'\n",
      "    spatial_ref  int64 8B 0\n",
      "Data variables:\n",
      "    NDVI         (epi_week, y, x) float32 444MB dask.array<chunksize=(1, 200, 200), meta=np.ndarray>\n",
      "    NDWI         (epi_week, y, x) float32 444MB dask.array<chunksize=(1, 200, 200), meta=np.ndarray>\n",
      "Attributes:\n",
      "    TIFFTAG_XRESOLUTION:     1\n",
      "    TIFFTAG_YRESOLUTION:     1\n",
      "    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n",
      "    AREA_OR_POINT:           Area\n",
      "    _FillValue:              0.0\n",
      "    scale_factor:            1.0\n",
      "    add_offset:              0.0\n",
      "‚úÖguardada: D:/OneDrive - CGIAR/Desktop/downscaling/processed/gee\\gee_2013-22_2025-05.nc\n"
     ]
    }
   ],
   "source": [
    "# ---------- Agregaci√≥n semana epidemiologica Google Earth Engine ----------\n",
    "\n",
    "GEE_NDVI = Path(r\"D:/OneDrive - CGIAR/Desktop/codigos_dengue/climate_data_downloader/data/processed/gee/NDVI\")\n",
    "GEE_NDWI = Path(r\"D:/OneDrive - CGIAR/Desktop/codigos_dengue/climate_data_downloader/data/processed/gee/NDWI\")\n",
    "GEE_OUTPUT= \"D:/OneDrive - CGIAR/Desktop/downscaling/data/preprocessed/gee\"\n",
    "shape = (433, 422) ## primero Y y luego X\n",
    "\n",
    "files_ndvi = list(GEE_NDVI.glob(\"*.tif\"))\n",
    "files_ndwi = list(GEE_NDWI.glob(\"*.tif\"))\n",
    "\n",
    "\n",
    "NDVI = process_gee(GEE_NDVI, files_ndvi, shape)\n",
    "NDWI = process_gee(GEE_NDWI, files_ndwi, shape)\n",
    "\n",
    "# Unir datasets en uno solo\n",
    "combined_dataset = xr.merge([NDVI, NDWI])\n",
    "print(combined_dataset)\n",
    "\n",
    "epi_weeks = combined_dataset.epi_week.values\n",
    "\n",
    "# Convertir a tuplas de enteros: (a√±o, semana)\n",
    "epi_tuples = pd.Series(epi_weeks).str.extract(r'(?P<year>\\d{4})-(?P<week>\\d{2})').astype(int)\n",
    "epi_tuples_list = list(zip(epi_tuples['year'], epi_tuples['week']))\n",
    "\n",
    "# Ordenar por a√±o y semana\n",
    "epi_tuples_sorted = sorted(epi_tuples_list)\n",
    "\n",
    "# Obtener m√≠nimo y m√°ximo como strings 'YYYY-WW'\n",
    "epi_min = f\"{epi_tuples_sorted[0][0]}-{epi_tuples_sorted[0][1]:02d}\"\n",
    "epi_max = f\"{epi_tuples_sorted[-1][0]}-{epi_tuples_sorted[-1][1]:02d}\"\n",
    "\n",
    "filename = os.path.join(GEE_OUTPUT, f\"gee_{epi_min}_{epi_max}.nc\") \n",
    "\n",
    "combined_dataset.to_netcdf(filename, format=\"NETCDF4\", engine=\"netcdf4\")\n",
    "\n",
    "print(f\"‚úÖguardada: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
