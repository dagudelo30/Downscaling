{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c3e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import xarray as xr\n",
    "import rioxarray\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dbdbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### funciones\n",
    "\n",
    "def start_of_epi_year(Y: int) -> pd.Timestamp:\n",
    "    dec29 = pd.Timestamp(Y - 1, 12, 29)\n",
    "    jan4  = pd.Timestamp(Y, 1, 4)\n",
    "    w = dec29.weekday()\n",
    "    offset = (6 - w) % 7\n",
    "    first_sunday = dec29 + pd.Timedelta(days=offset)\n",
    "    if first_sunday > jan4:\n",
    "        first_sunday -= pd.Timedelta(days=7)\n",
    "    return first_sunday\n",
    "\n",
    "\n",
    "\n",
    "def fecha_a_nombre(fecha: pd.Timestamp) -> str:\n",
    "    \"\"\"Convierte fecha calendario a nombre YYYYDDD.nc\"\"\"\n",
    "    doy = fecha.timetuple().tm_yday\n",
    "    return f\"{fecha.year}{doy:03d}.nc\"\n",
    "def get_epi_year_week(date: pd.Timestamp) -> tuple[int, int]:\n",
    "    date = pd.Timestamp(date)\n",
    "    # ‚Äúdomingo epidemiol√≥gico‚Äù de la fecha\n",
    "    offset = (date.weekday() + 1) % 7          # 0 = domingo\n",
    "    sunday = date - pd.Timedelta(days=offset)\n",
    "\n",
    "    next_start = start_of_epi_year(sunday.year + 1)\n",
    "    epi_year   = sunday.year if sunday < next_start else sunday.year + 1\n",
    "\n",
    "    start_year = start_of_epi_year(epi_year)\n",
    "    epi_week   = ((sunday - start_year).days // 7) + 1\n",
    "    return epi_year, epi_week\n",
    "\n",
    "\n",
    "def process_mswep(path: str,\n",
    "                  start_date: str | pd.Timestamp,\n",
    "                  end_date: str | pd.Timestamp,\n",
    "                  path_out: str,\n",
    "                  var_name: str = \"precipitation\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Procesa archivos NetCDF diarios MSWEP ya recortados en path,\n",
    "    entre start_date y end_date (ej: \"2013-06-01\" a \"2025-01-31\").\n",
    "\n",
    "    - Ignora archivos da√±ados o con coordenadas duplicadas.\n",
    "    - Combina archivos v√°lidos.\n",
    "    - Acumula por semana epidemiol√≥gica (coord: epi_week).\n",
    "    - Guarda un NetCDF comprimido en path_out con engine=\"h5netcdf\".\n",
    "\n",
    "    Devuelve un Dataset con dims (\"epi_week\", \"y\", \"x\").\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # --- Convertir fechas a rango ---\n",
    "    fechas = pd.date_range(start=start_date, end=end_date, freq=\"D\")\n",
    "    expected_files = {fecha_a_nombre(dt): dt for dt in fechas}\n",
    "    print(f\"üìÖ Procesando rango {start_date} ‚Üí {end_date} ({len(fechas)} d√≠as)\")\n",
    "\n",
    "    # --- Buscar archivos existentes ---\n",
    "    available = {f: os.path.join(path, f) for f in os.listdir(path) if f.endswith((\".nc\",\".nc4\"))}\n",
    "    target_files, missing_files = [], []\n",
    "\n",
    "    for fname, dt in expected_files.items():\n",
    "        if fname in available:\n",
    "            target_files.append(available[fname])\n",
    "        else:\n",
    "            missing_files.append(fname)\n",
    "\n",
    "    print(f\"‚úÖ {len(target_files)} archivos encontrados, ‚ùå {len(missing_files)} faltantes\")\n",
    "\n",
    "    # --- Validar archivos (descartar duplicados/da√±ados) ---\n",
    "    valid_files, bad_files = [], []\n",
    "    for f in target_files:\n",
    "        try:\n",
    "            with xr.open_dataset(f, engine=\"h5netcdf\") as ds:\n",
    "                if \"lon\" in ds.coords and len(np.unique(ds[\"lon\"])) < ds.sizes[\"lon\"]:\n",
    "                    bad_files.append((f, \"lon duplicado\"))\n",
    "                elif \"lat\" in ds.coords and len(np.unique(ds[\"lat\"])) < ds.sizes[\"lat\"]:\n",
    "                    bad_files.append((f, \"lat duplicado\"))\n",
    "                else:\n",
    "                    valid_files.append(f)\n",
    "        except Exception as e:\n",
    "            bad_files.append((f, f\"error: {str(e)}\"))\n",
    "\n",
    "    print(f\"üìÇ Usando {len(valid_files)} archivos v√°lidos, ignorados {len(bad_files)}\")\n",
    "    if bad_files:\n",
    "        for f, reason in bad_files:\n",
    "            print(f\"   ‚ö†Ô∏è {os.path.basename(f)} ‚Üí {reason}\")\n",
    "\n",
    "    if not valid_files:\n",
    "        raise RuntimeError(\"No se encontr√≥ ning√∫n archivo v√°lido para procesar.\")\n",
    "\n",
    "    # --- Preproceso individual ---\n",
    "    def preprocess(ds: xr.Dataset) -> xr.Dataset:\n",
    "        rn = {}\n",
    "        for d in ds.dims:\n",
    "            dl = d.lower()\n",
    "            if dl in (\"longitude\", \"x\"): rn[d] = \"x\"\n",
    "            if dl in (\"latitude\",  \"y\"): rn[d] = \"y\"\n",
    "        if rn:\n",
    "            ds = ds.rename(rn)\n",
    "\n",
    "        v = var_name if var_name in ds.data_vars else list(ds.data_vars)[0]\n",
    "        ds = ds[[v]]\n",
    "\n",
    "        # Garantizar 'time'\n",
    "        if \"time\" not in ds.dims:\n",
    "            src = ds.encoding.get(\"source\", None)\n",
    "            m = re.search(r'(\\d{4})(\\d{3})\\.nc', os.path.basename(src)) if src else None\n",
    "            if m:\n",
    "                year, doy = int(m.group(1)), int(m.group(2))\n",
    "                t = pd.to_datetime(f\"{year}{doy:03d}\", format=\"%Y%j\")\n",
    "                ds = ds.expand_dims(time=[t])\n",
    "            else:\n",
    "                raise ValueError(f\"No se pudo inferir 'time' en {src}\")\n",
    "        return ds.transpose(\"time\", ...)\n",
    "\n",
    "    # --- Abrir todos los archivos v√°lidos ---\n",
    "    ds_all = xr.open_mfdataset(\n",
    "        valid_files,\n",
    "        engine=\"h5netcdf\",\n",
    "        preprocess=preprocess,\n",
    "        combine=\"nested\",\n",
    "        concat_dim=\"time\",\n",
    "        parallel=False,\n",
    "        chunks={\"time\": 50},\n",
    "        data_vars=\"minimal\",\n",
    "        coords=\"minimal\",\n",
    "        compat=\"override\",\n",
    "    ).sortby(\"time\")\n",
    "\n",
    "    # --- Ordenar dims ---\n",
    "    for v in ds_all.data_vars:\n",
    "        dims = ds_all[v].dims\n",
    "        order = [d for d in (\"time\", \"y\", \"x\") if d in dims] + [d for d in dims if d not in (\"time\",\"y\",\"x\")]\n",
    "        ds_all[v] = ds_all[v].transpose(*order)\n",
    "\n",
    "    # --- Semana epidemiol√≥gica ---\n",
    "    times = pd.to_datetime(ds_all.time.values)\n",
    "    pairs = np.array([get_epi_year_week(t) for t in times])\n",
    "    labels = np.array([f\"{y}-{w:02d}\" for y, w in pairs])\n",
    "    ds_epi = ds_all.assign_coords(epi_week=(\"time\", labels))\n",
    "\n",
    "    # --- Acumulado semanal ---\n",
    "    weekly_sum = ds_epi.groupby(\"epi_week\").sum(\"time\", skipna=True)\n",
    "\n",
    "    if isinstance(weekly_sum, xr.DataArray):\n",
    "        ds_final = weekly_sum.to_dataset(name=var_name)\n",
    "    else:\n",
    "        ds_final = weekly_sum.rename({list(weekly_sum.data_vars)[0]: var_name})\n",
    "\n",
    "    # --- Nombre salida ---\n",
    "    epi_labels = sorted(set(labels))\n",
    "    epi_min, epi_max = epi_labels[0], epi_labels[-1]\n",
    "    filename = os.path.join(path_out, f\"mswep_{epi_min}_{epi_max}.nc\")\n",
    "\n",
    "    comp = dict(zlib=True, complevel=4)\n",
    "    encoding = {v: comp for v in ds_final.data_vars}\n",
    "    ds_final.to_netcdf(filename, format=\"NETCDF4\", engine=\"h5netcdf\", encoding=encoding)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Archivo guardado: {filename}\")\n",
    "    print(f\"üïí Tiempo total: {elapsed:.2f} segundos ({elapsed/60:.2f} minutos)\")\n",
    "    return ds_final\n",
    "\n",
    "def process_c3s(\n",
    "    path: str,\n",
    "    start_date: str | pd.Timestamp,\n",
    "    end_date: str | pd.Timestamp,\n",
    "    vars: dict[str, str],           # {'tmean': '2M_TEMPERATURE_MEAN', ...}\n",
    "    path_out: str,\n",
    "    crs: str = \"EPSG:4326\",\n",
    ") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Une d√≠a por d√≠a los NetCDF de C3S que est√°n separados por carpeta/variable.\n",
    "    Devuelve un Dataset con todas las variables concatenadas a lo largo de 'time'.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    path = Path(path)\n",
    "    fechas = pd.date_range(pd.Timestamp(start_date), pd.Timestamp(end_date), freq=\"D\")\n",
    "\n",
    "    diarios = []\n",
    "    print(f\"üì• Procesando {len(fechas)} fechas entre {start_date} y {end_date} desde: {path}\")\n",
    "    \n",
    "    for dt in tqdm(fechas, desc=\"Procesando archivos C3S\", unit=\"d√≠a\"):\n",
    "        fecha_str = dt.strftime(\"%Y%m%d\")\n",
    "        datasets_del_dia = []\n",
    "\n",
    "        for alias, carpeta in vars.items():\n",
    "            folder = path / carpeta\n",
    "            patron = f\"*{fecha_str}*\"\n",
    "            archivos = list(folder.rglob(patron))\n",
    "\n",
    "            if not archivos:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"‚õî No se encontr√≥ archivo con fecha {fecha_str} en {folder}\"\n",
    "                )\n",
    "\n",
    "            ds = xr.open_dataset(archivos[0], decode_coords=\"all\")\n",
    "            var_name = list(ds.data_vars)[0]\n",
    "            ds = ds.rename({var_name: alias})\n",
    "\n",
    "            if \"time\" not in ds.dims:\n",
    "                ds = ds.expand_dims(time=[dt])\n",
    "            else:\n",
    "                ds = ds.assign_coords(time=(\"time\", [dt]))\n",
    "\n",
    "            datasets_del_dia.append(ds)\n",
    "\n",
    "        ds_diario = xr.merge(datasets_del_dia, combine_attrs=\"override\")\n",
    "        diarios.append(ds_diario)\n",
    "\n",
    "    datos = xr.concat(diarios, dim=\"time\").sortby(\"time\")\n",
    "\n",
    "    # Asignar semanas epidemiol√≥gicas\n",
    "    epi_weeks = []\n",
    "    for ts in datos.time.dt.floor(\"D\").values:\n",
    "        epi_year, epi_week = get_epi_year_week(ts)\n",
    "        epi_weeks.append(f\"{epi_year}-{epi_week:02d}\")\n",
    "\n",
    "    datos_epi = datos.assign_coords(epi_week=(\"time\", epi_weeks))\n",
    "    c3s_mean = datos_epi.groupby([\"epi_week\"]).mean(\"time\")\n",
    "\n",
    "    if crs:\n",
    "        try:\n",
    "            c3s_mean = c3s_mean.rio.write_crs(crs, inplace=False)\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    # Obtener rango de semanas para el nombre del archivo\n",
    "    epi_tuples = pd.Series(epi_weeks).str.extract(r'(?P<year>\\d{4})-(?P<week>\\d{2})').astype(int)\n",
    "    epi_tuples_list = list(zip(epi_tuples['year'], epi_tuples['week']))\n",
    "    epi_tuples_sorted = sorted(epi_tuples_list)\n",
    "    epi_min = f\"{epi_tuples_sorted[0][0]}-{epi_tuples_sorted[0][1]:02d}\"\n",
    "    epi_max = f\"{epi_tuples_sorted[-1][0]}-{epi_tuples_sorted[-1][1]:02d}\"\n",
    "    \n",
    "    filename = os.path.join(path_out, f\"c3s_{epi_min}_{epi_max}.nc\") \n",
    "    c3s_mean.to_netcdf(filename, format=\"NETCDF4\", engine=\"netcdf4\")\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Archivo guardado: {filename}\")\n",
    "    print(f\"üïí Tiempo total: {elapsed:.2f} segundos ({elapsed/60:.2f} minutos)\")\n",
    "\n",
    "    return c3s_mean\n",
    "\n",
    "\n",
    "def process_gee(\n",
    "    path: Path,\n",
    "    files: list[Path],\n",
    "    shape: tuple[int, int],\n",
    "    crs: str = \"EPSG:4326\"\n",
    ") -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Procesa una lista de archivos GeoTIFF diarios (NDVI, NDWI, etc.) provenientes de GEE.\n",
    "    Agrega coordenadas temporales y CRS, luego agrega semanalmente por semana epidemiol√≥gica.\n",
    "    \n",
    "    Par√°metros:\n",
    "    - path: Carpeta contenedora (solo usada para nombrar la variable final).\n",
    "    - files: Lista de archivos GeoTIFF (uno por d√≠a).\n",
    "    - shape: Dimensiones esperadas de los datos (alto, ancho).\n",
    "    - crs: Sistema de referencia de coordenadas a usar.\n",
    "\n",
    "    Retorna:\n",
    "    - xr.DataArray con dimensi√≥n 'epi_week' y nombre igual al nombre de la carpeta.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    if not files:\n",
    "        raise ValueError(f\"No se proporcionaron archivos para {path}\")\n",
    "\n",
    "    print(f\"üìÇ Procesando {len(files)} archivos desde: {path}\")\n",
    "\n",
    "    # Obtener shape y coordenadas de referencia\n",
    "    ref_coords_found = False\n",
    "    for f_ref in files:\n",
    "        try:\n",
    "            ds_ref = rioxarray.open_rasterio(f_ref).squeeze(\"band\", drop=True)\n",
    "            if ds_ref.shape == shape:\n",
    "                ref_x, ref_y = ds_ref.x.values, ds_ref.y.values\n",
    "                ref_coords_found = True\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {f_ref.name}: {e}\")\n",
    "\n",
    "    if not ref_coords_found:\n",
    "        raise RuntimeError(\"No se encontr√≥ archivo con shape v√°lido para usar como referencia.\")\n",
    "\n",
    "    datasets = []\n",
    "    archivos_excluidos = []\n",
    "\n",
    "    for f in tqdm(files, desc=\"Procesando GeoTIFFs GEE\", unit=\"archivo\"):\n",
    "        fecha = extraer_fecha_individual(f)\n",
    "        try:\n",
    "            ds = rioxarray.open_rasterio(f, chunks={\"y\": 200, \"x\": 200}).squeeze(\"band\", drop=True)\n",
    "            if ds.shape == shape:\n",
    "                ds = ds.reset_coords(drop=True)\n",
    "                ds = ds.assign_coords({\"x\": ref_x, \"y\": ref_y})\n",
    "                ds = ds.expand_dims(time=[fecha])\n",
    "                datasets.append(ds)\n",
    "            else:\n",
    "                print(f\"‚õî Excluido (dimensiones incorrectas {ds.shape}): {f.name}\")\n",
    "                archivos_excluidos.append(f.name)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {f.name}: {e}\")\n",
    "            archivos_excluidos.append(f.name)\n",
    "\n",
    "    if not datasets:\n",
    "        raise RuntimeError(\"No se pudo cargar ning√∫n archivo con dimensiones v√°lidas.\")\n",
    "\n",
    "    da_concat = xr.concat(datasets, dim=\"time\")\n",
    "\n",
    "    epi_weeks = []\n",
    "    for ts in da_concat.time.dt.floor(\"D\").values:\n",
    "        epi_year, epi_week = get_epi_year_week(ts)\n",
    "        epi_weeks.append(f\"{epi_year}-{epi_week:02d}\")\n",
    "\n",
    "    datos_epi = da_concat.assign_coords(epi_week=(\"time\", epi_weeks))\n",
    "    datos_mean = datos_epi.groupby(\"epi_week\").mean(\"time\")\n",
    "\n",
    "    if crs:\n",
    "        try:\n",
    "            datos_mean = datos_mean.rio.write_crs(crs, inplace=False)\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    variable_name = path.name\n",
    "    datos_mean.name = variable_name\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ {variable_name}: {len(datasets)} archivos incluidos, {len(archivos_excluidos)} excluidos.\")\n",
    "    print(f\"üïí Tiempo total: {elapsed:.2f} segundos ({elapsed/60:.2f} minutos)\")\n",
    "\n",
    "    return datos_mean\n",
    "\n",
    "\n",
    "def extraer_fecha_individual(f: Path) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Extrae la fecha desde el nombre del archivo, buscando un patr√≥n YYYY-MM-DD o YYYYMMDD.\n",
    "    Devuelve un pd.Timestamp.\n",
    "    \"\"\"\n",
    "    fname = f.name\n",
    "    # Intenta YYYY-MM-DD\n",
    "    match = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", fname)\n",
    "    if match:\n",
    "        return pd.to_datetime(match.group(0))\n",
    "    \n",
    "    # Intenta YYYYMMDD\n",
    "    match = re.search(r\"\\d{8}\", fname)\n",
    "    if match:\n",
    "        return pd.to_datetime(match.group(0), format=\"%Y%m%d\")\n",
    "    \n",
    "    raise ValueError(f\"No se pudo extraer fecha del nombre: {fname}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b900944",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fecha_a_nombre' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m MSWEP_IN  \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSebasti√°n Palomino\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmswep_files_valle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m MSWEP_OUT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSebasti√°n Palomino\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDown\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproccesed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m mswep \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_mswep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMSWEP_IN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2013-06-01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2025-01-31\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMSWEP_OUT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(mswep)\n",
      "Cell \u001b[1;32mIn[2], line 49\u001b[0m, in \u001b[0;36mprocess_mswep\u001b[1;34m(path, start_date, end_date, path_out, var_name)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# --- Convertir fechas a rango ---\u001b[39;00m\n\u001b[0;32m     48\u001b[0m fechas \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mdate_range(start\u001b[38;5;241m=\u001b[39mstart_date, end\u001b[38;5;241m=\u001b[39mend_date, freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m expected_files \u001b[38;5;241m=\u001b[39m {\u001b[43mfecha_a_nombre\u001b[49m(dt): dt \u001b[38;5;28;01mfor\u001b[39;00m dt \u001b[38;5;129;01min\u001b[39;00m fechas}\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìÖ Procesando rango \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(fechas)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m d√≠as)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# --- Buscar archivos existentes ---\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fecha_a_nombre' is not defined"
     ]
    }
   ],
   "source": [
    "# ---------- Agregaci√≥n semana epidemiologica MSWEP ----------\n",
    "MSWEP_IN  = r\"C:\\Users\\Sebasti√°n Palomino\\Downloads\\mswep_files_valle\"\n",
    "MSWEP_OUT = r\"C:\\Users\\Sebasti√°n Palomino\\Desktop\\Down\\data\\proccesed\"\n",
    "mswep = process_mswep(MSWEP_IN, \"2013-06-01\", \"2025-01-31\", MSWEP_OUT)\n",
    "print(mswep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfbe0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Procesando 4263 fechas entre 2013-06-01 y 2025-01-31 desde: D:\\OneDrive - CGIAR\\Desktop\\codigos_dengue\\climate_data_downloader\\data\\processed\\copernicus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos C3S: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4263/4263 [13:50<00:00,  5.13d√≠a/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Archivo guardado: D:/OneDrive - CGIAR/Desktop/downscaling/processed/copernicus\\c3s_2013-22_2025-05.nc\n",
      "üïí Tiempo total: 901.42 segundos (15.02 minutos)\n",
      "<xarray.Dataset> Size: 3MB\n",
      "Dimensions:      (epi_week: 610, lat: 20, lon: 17)\n",
      "Coordinates:\n",
      "  * lon          (lon) float64 136B -77.4 -77.3 -77.2 ... -76.0 -75.9 -75.8\n",
      "  * lat          (lat) float64 160B 5.0 4.9 4.8 4.7 4.6 ... 3.5 3.4 3.3 3.2 3.1\n",
      "  * epi_week     (epi_week) object 5kB '2013-22' '2013-23' ... '2025-05'\n",
      "    spatial_ref  int64 8B 0\n",
      "Data variables:\n",
      "    tmax         (epi_week, lat, lon) float32 830kB 301.0 302.7 ... 287.0 290.3\n",
      "    tmin         (epi_week, lat, lon) float32 830kB 296.5 295.1 ... 280.5 282.8\n",
      "    humidity     (epi_week, lat, lon) float32 830kB 82.11 86.37 ... 92.78 91.18\n",
      "    windspeed    (epi_week, lat, lon) float32 830kB 2.703 1.769 ... 1.564 1.246\n",
      "Attributes:\n",
      "    CDI:          Climate Data Interface version 1.9.2 (http://mpimet.mpg.de/...\n",
      "    history:      Fri Mar 12 15:16:22 2021: cdo splitday /archive/ESG/wit015/...\n",
      "    Conventions:  CF-1.7\n",
      "    CDO:          Climate Data Operators version 1.9.2 (http://mpimet.mpg.de/...\n"
     ]
    }
   ],
   "source": [
    "# ---------- Agregaci√≥n semana epidemiologica Copernicus ----------\n",
    "C3S_VARIABLES = {\n",
    "    'tmax' : '2M_TEMPERATURE_MAXIMUM',\n",
    "    'tmin' : '2M_TEMPERATURE_MINIMUM',\n",
    "    'humidity': '2M_RELATIVE_HUMIDITY',\n",
    "    'windspeed': '10M_WIND_SPEED'}\n",
    "C3S_INPUT = \"D:/OneDrive - CGIAR/Desktop/codigos_dengue/climate_data_downloader/data/processed/copernicus\"\n",
    "C3S_OUTPUT= \"D:/OneDrive - CGIAR/Desktop/downscaling/data/preprocessed/copernicus\"\n",
    "c3s  = process_c3s(C3S_INPUT,\"2013-06-01\", \"2025-01-31\",C3S_VARIABLES,C3S_OUTPUT)\n",
    "print(c3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8896fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Procesando 4242 archivos desde: D:\\OneDrive - CGIAR\\Desktop\\codigos_dengue\\climate_data_downloader\\data\\processed\\gee\\NDVI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs GEE:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1943/4242 [00:35<00:41, 55.78archivo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚õî Excluido (dimensiones incorrectas (433, 421)): NDVI_MODIS_2018-09-27.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs GEE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4242/4242 [01:19<00:00, 53.67archivo/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ NDVI: 4241 archivos incluidos, 1 excluidos.\n",
      "üïí Tiempo total: 82.48 segundos (1.37 minutos)\n",
      "üìÇ Procesando 4241 archivos desde: D:\\OneDrive - CGIAR\\Desktop\\codigos_dengue\\climate_data_downloader\\data\\processed\\gee\\NDWI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs GEE:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1946/4241 [00:35<00:38, 59.79archivo/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚õî Excluido (dimensiones incorrectas (433, 421)): NDWI_MODIS_2018-09-27.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando GeoTIFFs GEE: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4241/4241 [01:17<00:00, 55.05archivo/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ NDWI: 4240 archivos incluidos, 1 excluidos.\n",
      "üïí Tiempo total: 79.42 segundos (1.32 minutos)\n",
      "<xarray.Dataset> Size: 889MB\n",
      "Dimensions:      (x: 422, y: 433, epi_week: 608)\n",
      "Coordinates:\n",
      "  * x            (x) float64 3kB -8.601e+06 -8.6e+06 ... -8.391e+06 -8.39e+06\n",
      "  * y            (y) float64 3kB 5.598e+05 5.593e+05 ... 3.443e+05 3.438e+05\n",
      "  * epi_week     (epi_week) object 5kB '2013-22' '2013-23' ... '2025-05'\n",
      "    spatial_ref  int64 8B 0\n",
      "Data variables:\n",
      "    NDVI         (epi_week, y, x) float32 444MB dask.array<chunksize=(1, 200, 200), meta=np.ndarray>\n",
      "    NDWI         (epi_week, y, x) float32 444MB dask.array<chunksize=(1, 200, 200), meta=np.ndarray>\n",
      "Attributes:\n",
      "    TIFFTAG_XRESOLUTION:     1\n",
      "    TIFFTAG_YRESOLUTION:     1\n",
      "    TIFFTAG_RESOLUTIONUNIT:  1 (unitless)\n",
      "    AREA_OR_POINT:           Area\n",
      "    _FillValue:              0.0\n",
      "    scale_factor:            1.0\n",
      "    add_offset:              0.0\n",
      "‚úÖguardada: D:/OneDrive - CGIAR/Desktop/downscaling/processed/gee\\gee_2013-22_2025-05.nc\n"
     ]
    }
   ],
   "source": [
    "# ---------- Agregaci√≥n semana epidemiologica Google Earth Engine ----------\n",
    "\n",
    "GEE_NDVI = Path(r\"D:/OneDrive - CGIAR/Desktop/codigos_dengue/climate_data_downloader/data/processed/gee/NDVI\")\n",
    "GEE_NDWI = Path(r\"D:/OneDrive - CGIAR/Desktop/codigos_dengue/climate_data_downloader/data/processed/gee/NDWI\")\n",
    "GEE_OUTPUT= \"D:/OneDrive - CGIAR/Desktop/downscaling/data/preprocessed/gee\"\n",
    "shape = (433, 422) ## primero Y y luego X\n",
    "\n",
    "files_ndvi = list(GEE_NDVI.glob(\"*.tif\"))\n",
    "files_ndwi = list(GEE_NDWI.glob(\"*.tif\"))\n",
    "\n",
    "\n",
    "NDVI = process_gee(GEE_NDVI, files_ndvi, shape)\n",
    "NDWI = process_gee(GEE_NDWI, files_ndwi, shape)\n",
    "\n",
    "# Unir datasets en uno solo\n",
    "combined_dataset = xr.merge([NDVI, NDWI])\n",
    "print(combined_dataset)\n",
    "\n",
    "epi_weeks = combined_dataset.epi_week.values\n",
    "\n",
    "# Convertir a tuplas de enteros: (a√±o, semana)\n",
    "epi_tuples = pd.Series(epi_weeks).str.extract(r'(?P<year>\\d{4})-(?P<week>\\d{2})').astype(int)\n",
    "epi_tuples_list = list(zip(epi_tuples['year'], epi_tuples['week']))\n",
    "\n",
    "# Ordenar por a√±o y semana\n",
    "epi_tuples_sorted = sorted(epi_tuples_list)\n",
    "\n",
    "# Obtener m√≠nimo y m√°ximo como strings 'YYYY-WW'\n",
    "epi_min = f\"{epi_tuples_sorted[0][0]}-{epi_tuples_sorted[0][1]:02d}\"\n",
    "epi_max = f\"{epi_tuples_sorted[-1][0]}-{epi_tuples_sorted[-1][1]:02d}\"\n",
    "\n",
    "filename = os.path.join(GEE_OUTPUT, f\"gee_{epi_min}_{epi_max}.nc\") \n",
    "\n",
    "combined_dataset.to_netcdf(filename, format=\"NETCDF4\", engine=\"netcdf4\")\n",
    "\n",
    "print(f\"‚úÖguardada: {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
